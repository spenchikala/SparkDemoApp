
---------------------------------------------------------------------------------------------------
//
//  Launch Spark Scala Shell:
//

export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_60.jdk/Contents/Home
export PATH=$PATH:$JAVA_HOME/bin

java -version

export M2_HOME=/Users/srini/dev/apache-maven-3.2.1
export PATH=$PATH:$M2_HOME/bin

mvn -version

export SCALA_HOME=/Users/srini/dev/scala-2.11.1
export PATH=$PATH:$SCALA_HOME/bin

scala -version

export SBT_HOME=/Users/srini/dev/sbt
export PATH=$PATH:$SBT_HOME/bin

export PYTHON_HOME=/Users/srini/dev/anaconda/bin/
export PATH=$PATH:$PYTHON_HOME/bin

export MAVEN_OPTS=-Xmx1024m -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=512m

cd /Users/srini/dev/spark-1.0.0

./bin/spark-shell


---------------------------------------------------------------------------------------------------
scala

val myNumbers = List(1, 2, 5, 4, 7, 3)

def cube(a: Int): Int = a * a * a

myNumbers.map(x => cube(x))

or

myNumbers.map(cube(_))

or

myNumbers.map(cube)


myNumbers.map{x => x * x * x}


def factorial(n:Int):Int = if (n==0) 1 else n * factorial(n-1) // From http://bit.ly/b2sVKI

myNumbers.map(factorial).sum


import scala.io.Source
val lines = Source.fromFile("README.md").getLines.toArray

val counts = new collection.mutable.HashMap[String, Int].withDefaultValue(0)

counts

lines.flatMap(line => line.split(" ")).foreach(word => counts(word) += 1)

counts


import scala.io.Source
val lines = Source.fromFile("README.md").getLines.toArray

val emptyCounts = Map[String,Int]().withDefaultValue(0)

val words = lines.flatMap(line => line.split(" "))

val counts = words.foldLeft(emptyCounts)({(currentCounts: Map[String,Int], word: String) => currentCounts.updated(word, currentCounts(word) + 1)})

counts

---------------------------------------------------------------------------------------------------
//
// Spark Examples
//

//
// Tomcat logs example.
//
var logs = sc.textFile("c:/dev/servers/apache-tomcat-7.0.42/logs/")

//logs.count

val errors = logs.filter(_.contains("ERROR"))

val seqad = errors.filter(_.contains("SequenceAdmin"))


//
// parallelized collection example
// example used Scala interpreter (also called 'repl') as an interface to Spark
//

//
// Scala Example # 1
//
sc.parallelize(1 to 1000).count()


//
// Scala Example # 2
//

// declare scala array:
val data = Array(1, 2, 3, 4, 5)

// distribute the array in a cluster:
val distData = sc.parallelize(data)

// sc is an instance of SparkCluster that's initialized for you by Scala repl
// returns Resilient Distributed Dataset:
Output:
distData: spark.RDD[Int] = spark.ParallelCollection@88eebe8e  // repl output


//
// Scala Example # 3 - PI Example
//

val NUM_SAMPLES = 100000
val count = sc.parallelize(1 to NUM_SAMPLES).map{i =>
	val x = Math.random * 2 - 1
	val y = Math.random * 2 - 1
	if (x * x + y * y < 1) 1.0 else 0.0
	}.reduce(_ + _)
println("Pi is roughly " + 4 * count / NUM_SAMPLES)


//
// Scala Example # 4
//
val textFile = sc.textFile("README.md")

textFile.count() // Number of items in this RDD

textFile.first() // First item in this RDD

val linesWithSpark = textFile.filter(line => line.contains("Spark"))

linesWithSpark.count()

or

textFile.filter(line => line.contains("Spark")).count() // How many lines contain "Spark"?


// RDD Operations

// RDD actions and transformations can be used for more complex computations.
// If we want to find the line with the most words:

textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)


// Using Math Library

import java.lang.Math
textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))

//
//  Spark can implement MapReduce flows easily:
//
val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey((a, b) => a + b)


// To collect the word counts in our shell, we can use the collect action:
wordCounts.collect()

//
// Caching
//

linesWithSpark.cache()

// Call the count method.
linesWithSpark.count()

// Call the count method again. This time, it should take lot less time because the data is already in the cache.
linesWithSpark.count()


//
// To exit Scala Shell 
//
:q

---------------------------------------------------------------------------------------------------
//
//  Launch Spark Scala Shell:
//

//
// Interactive Analysis
//

sc


// val pagecounts = sc.textFile("c:/dev/servers/spark-1.0.0/README.md")

val pagecounts = sc.textFile("c:/dev/temp/pagecounts-20140601-000000")

pagecounts.count

pagecounts.take(10)

pagecounts.take(10).foreach(println)

//
// To avoid reading from disks each time we perform any operations on the RDD, 
// we also cache the RDD into memory. This is where Spark really starts to to shine.
//

val enPages = pagecounts.filter(_.split(" ")(1) == "en").cache

enPages.count

val enTuples = enPages.map(line => line.split(" "))

val enKeyValuePairs = enTuples.map(line => (line(0).substring(0, 8), line(3).toInt))

enKeyValuePairs.reduceByKey(_+_, 1).collect

enPages.map(line => line.split(" ")).map(line => (line(0).substring(0, 8), line(3).toInt)).reduceByKey(_+_, 1).collect

enPages.map(l => l.split(" ")).map(l => (l(2), l(3).toInt)).reduceByKey(_+_, 40).filter(x => x._2 > 200000).map(x => (x._2, x._1)).collect.foreach(println)


